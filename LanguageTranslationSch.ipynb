{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageTranslationSch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadlamani-s/Schneider/blob/main/LanguageTranslationSch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjedieXh8-8i",
        "outputId": "35cc7f7d-3165-45ed-a909-7fe2ccd8014e"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 25.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3040770 sha256=3422d64f0632d8b58b4f5d5a3302a7cefb6319a19f740e08745061c136490c11\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSUC-am59LKo"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "from keras import initializers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "import fasttext.util\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PJazD5b1UJY"
      },
      "source": [
        "#Package for Language Identification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Ir6HYI9LWR",
        "outputId": "06aea436-6a74-4c37-9997-9c54f53a24d6"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 21:23:13--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M  50.5MB/s    in 2.5s    \n",
            "\n",
            "2020-11-23 21:23:16 (50.5 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k72KzYQn40qF"
      },
      "source": [
        "#French and Haitain Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu5I4WPOCBHx",
        "outputId": "0e790e77-a30b-4284-8984-7a7125b5d1df"
      },
      "source": [
        "#fasttext.util.download_model('fr', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.fr.300.bin')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nubQHB3zdLwS",
        "outputId": "9ad26a53-2dc7-4ed2-bac6-9bee2875ddb5"
      },
      "source": [
        "#fasttext.util.download_model('ht', if_exists='ignore')\n",
        "ft_ht = fasttext.load_model('cc.ht.300.bin')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KfVlGr347kW"
      },
      "source": [
        "#Language Identification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm8NPZ-3C94r",
        "outputId": "98c33c55-ea0e-469d-c0eb-ab129e10fb08"
      },
      "source": [
        "pretrained_lang_model = \"lid.176.bin\"\n",
        "model_language = fasttext.load_model(pretrained_lang_model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVaUKoMh3oAg"
      },
      "source": [
        "def reading_to_dataframe(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df = df.iloc[:,2:4].dropna()\n",
        "  df.reset_index(inplace=True, drop = True)\n",
        "  df['message'] = df['message'].str.lower()\n",
        "  df['original'] = df['original'].str.lower()\n",
        "  return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmoP5Zzt5au2",
        "outputId": "e0d34158-ffd8-43ae-ef81-0c11b6456594"
      },
      "source": [
        "train_path = \"/content/disaster_response_messages_training.csv\"\n",
        "validation_path = \"/content/disaster_response_messages_validation.csv\"\n",
        "test_path = \"/content/disaster_response_messages_test.csv\"\n",
        "df_train = reading_to_dataframe(train_path)\n",
        "df_validation = reading_to_dataframe(validation_path)\n",
        "df_test = reading_to_dataframe(test_path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9LPaDWPwjbe",
        "outputId": "fe62d15a-7c44-485a-a224-122a57ad40fa"
      },
      "source": [
        "df = reading_to_dataframe(train_path)\n",
        "df = df.append(df_validation)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euh-16ARSL2e"
      },
      "source": [
        "#Language Labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRtYHw3kRvyO",
        "outputId": "5860f7dd-12c1-498d-d04e-067e4ed59108"
      },
      "source": [
        "list_train = list()\n",
        "for sentence in df_train['original']:\n",
        "  list_train.append(model_language.predict(sentence, k = 2)[0][0])\n",
        "print(Counter(list_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'__label__fr': 2312, '__label__tl': 1160, '__label__ht': 972, '__label__en': 881, '__label__la': 213, '__label__ro': 205, '__label__hr': 168, '__label__pl': 158, '__label__es': 150, '__label__tr': 131, '__label__de': 127, '__label__ku': 115, '__label__nl': 109, '__label__pms': 104, '__label__wa': 103, '__label__jv': 91, '__label__eo': 88, '__label__br': 87, '__label__cy': 76, '__label__it': 73, '__label__su': 64, '__label__sq': 55, '__label__vi': 55, '__label__ca': 53, '__label__id': 49, '__label__pt': 46, '__label__sw': 43, '__label__war': 41, '__label__sl': 34, '__label__ms': 24, '__label__no': 23, '__label__hu': 20, '__label__ie': 20, '__label__ceb': 19, '__label__io': 18, '__label__sh': 17, '__label__kw': 16, '__label__fi': 14, '__label__fy': 14, '__label__et': 13, '__label__mg': 12, '__label__mt': 12, '__label__gn': 11, '__label__sr': 11, '__label__sv': 11, '__label__ur': 10, '__label__cs': 10, '__label__eu': 10, '__label__bs': 9, '__label__sc': 7, '__label__cbk': 7, '__label__az': 7, '__label__eml': 6, '__label__bcl': 6, '__label__ia': 6, '__label__sco': 5, '__label__lt': 5, '__label__diq': 5, '__label__nds': 5, '__label__ast': 4, '__label__da': 4, '__label__tk': 4, '__label__sk': 3, '__label__lv': 3, '__label__qu': 3, '__label__lb': 3, '__label__ta': 3, '__label__gl': 3, '__label__nap': 3, '__label__is': 3, '__label__lmo': 2, '__label__rm': 2, '__label__jbo': 2, '__label__so': 2, '__label__ilo': 2, '__label__nn': 2, '__label__mrj': 2, '__label__vo': 2, '__label__oc': 2, '__label__af': 2, '__label__dsb': 2, '__label__uz': 2, '__label__gu': 1, '__label__te': 1, '__label__als': 1, '__label__hif': 1, '__label__gv': 1, '__label__or': 1, '__label__th': 1, '__label__li': 1, '__label__an': 1, '__label__gd': 1, '__label__zh': 1, '__label__ko': 1, '__label__mk': 1, '__label__si': 1, '__label__mr': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVNhDGruRv7g",
        "outputId": "897e4ae9-5cbe-463a-91c2-a8a26d56e367"
      },
      "source": [
        "list_validation = list()\n",
        "for sentence in df_validation['original']:\n",
        "  list_validation.append(model_language.predict(sentence, k = 2)[0][0])\n",
        "print(Counter(list_validation))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'__label__fr': 287, '__label__tl': 132, '__label__en': 109, '__label__ht': 106, '__label__la': 32, '__label__es': 28, '__label__ro': 24, '__label__tr': 19, '__label__ku': 17, '__label__hr': 16, '__label__pl': 16, '__label__nl': 15, '__label__de': 13, '__label__wa': 13, '__label__it': 11, '__label__su': 11, '__label__sq': 11, '__label__eo': 10, '__label__br': 10, '__label__jv': 9, '__label__pms': 8, '__label__sw': 7, '__label__no': 6, '__label__pt': 6, '__label__id': 5, '__label__sl': 5, '__label__vi': 5, '__label__ca': 5, '__label__kw': 4, '__label__cy': 3, '__label__bcl': 3, '__label__fy': 3, '__label__ms': 3, '__label__cs': 2, '__label__ur': 2, '__label__gl': 2, '__label__eml': 2, '__label__sr': 2, '__label__diq': 2, '__label__so': 1, '__label__mwl': 1, '__label__ceb': 1, '__label__sh': 1, '__label__mg': 1, '__label__cbk': 1, '__label__sc': 1, '__label__gn': 1, '__label__mt': 1, '__label__is': 1, '__label__li': 1, '__label__ilo': 1, '__label__war': 1, '__label__fi': 1, '__label__mrj': 1, '__label__io': 1, '__label__mk': 1, '__label__eu': 1, '__label__hu': 1, '__label__ast': 1, '__label__nn': 1, '__label__sco': 1, '__label__sv': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDER723SRwEL",
        "outputId": "422683ce-d74c-46c2-b828-5f19f568cd14"
      },
      "source": [
        "list_test = list()\n",
        "for sentence in df_test['original']:\n",
        "  list_test.append(model_language.predict(sentence, k = 2)[0][0])\n",
        "print(Counter(list_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'__label__fr': 281, '__label__tl': 144, '__label__ht': 129, '__label__en': 98, '__label__la': 37, '__label__es': 30, '__label__ro': 25, '__label__hr': 24, '__label__de': 20, '__label__nl': 19, '__label__ku': 14, '__label__br': 14, '__label__pms': 12, '__label__wa': 11, '__label__eo': 11, '__label__su': 11, '__label__pl': 10, '__label__ca': 10, '__label__jv': 9, '__label__tr': 8, '__label__war': 7, '__label__sq': 7, '__label__it': 6, '__label__sw': 5, '__label__id': 5, '__label__vi': 4, '__label__sl': 4, '__label__cy': 4, '__label__ms': 3, '__label__ceb': 3, '__label__oc': 3, '__label__ie': 3, '__label__no': 3, '__label__fi': 3, '__label__gu': 2, '__label__gv': 2, '__label__pt': 2, '__label__gn': 2, '__label__eu': 2, '__label__hu': 2, '__label__fy': 1, '__label__ilo': 1, '__label__tk': 1, '__label__sh': 1, '__label__mg': 1, '__label__sco': 1, '__label__yo': 1, '__label__nds': 1, '__label__bs': 1, '__label__lmo': 1, '__label__mt': 1, '__label__mrj': 1, '__label__qu': 1, '__label__gom': 1, '__label__jbo': 1, '__label__kw': 1, '__label__zh': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0GuGgsqR5cG"
      },
      "source": [
        "Since, the highest number of sentences in the training, validation and test set are in the French and Haitian language, I used the pre trained French and Haitian embedding from fasttext library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re5dP3Vazo1-"
      },
      "source": [
        "Adding the labels to the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04T9gjmARwMU"
      },
      "source": [
        "df_train['label'] = list_train\n",
        "df_validation['label'] = list_validation\n",
        "df_test['label'] = list_test"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQlsbTixzyJ8"
      },
      "source": [
        "Removed all the other languages from training, validation and test sets apart from French and Haitain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaYuCeD0SR6y"
      },
      "source": [
        "df_train = df_train[(df_train['label'] == \"__label__fr\") | (df_train['label'] == \"__label__tl\")]\n",
        "df_train.reset_index(inplace=True, drop = True)\n",
        "df_validation = df_validation[(df_validation['label'] == \"__label__fr\") | (df_validation['label'] == \"__label__tl\")]\n",
        "df_validation.reset_index(inplace=True, drop = True)\n",
        "df_test = df_test[(df_test['label'] == \"__label__fr\") | (df_test['label'] == \"__label__tl\")]\n",
        "df_test.reset_index(inplace=True, drop = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeMHdt00UZUG"
      },
      "source": [
        "df = df_train\n",
        "df = df.append(df_validation)\n",
        "df.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNGQvAdF7j0M"
      },
      "source": [
        "#Tokenization of the Data using Keras, removing all the punctuations\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leMe9t2p7hy0"
      },
      "source": [
        "def tokenization(lines):\n",
        "  tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZlN6c6BCtTD"
      },
      "source": [
        "tokenizer_message_train = tokenization(df['message'])\n",
        "tokenizer_original_train = tokenization(df['original'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uYtWcZS83tA"
      },
      "source": [
        "vocabulary_size_original = len(tokenizer_original_train.word_index) + 1\n",
        "vocabulary_size_message = len(tokenizer_message_train.word_index) + 1\n",
        "max_original_sent_length = df['message'].str.split().str.len().max()\n",
        "max_message_sent_length = df['original'].str.split().str.len().max()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5mBAXku9fCT",
        "outputId": "009ece9d-9def-4a11-f7da-9e1cbc874613"
      },
      "source": [
        "print(\"Max vocabulary length of multi-lingual data :\", vocabulary_size_original)\n",
        "print(\"Max vocabulary length of english data :\", vocabulary_size_message)\n",
        "print(\"Max sentence length of multi-lingual data :\", max_original_sent_length)\n",
        "print(\"Max sentence length of english data :\", max_message_sent_length)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max vocabulary length of multi-lingual data : 9222\n",
            "Max vocabulary length of english data : 5947\n",
            "Max sentence length of multi-lingual data : 66\n",
            "Max sentence length of english data : 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXF4s-biFwQD"
      },
      "source": [
        "#Encoding sequences and padding with zero's\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOPsP_IhBCVP"
      },
      "source": [
        "def encode_sequences(tokenizer, max_length, sentences):\n",
        "  seq = tokenizer.texts_to_sequences(sentences)\n",
        "  seq = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d3pwUDsFmuW"
      },
      "source": [
        "encode_message = encode_sequences(tokenizer_message_train, max_message_sent_length, df_train['message'])\n",
        "encode_original = encode_sequences(tokenizer_original_train, max_original_sent_length, df_train['original'])\n",
        "encode_message_validate = encode_sequences(tokenizer_message_train, max_message_sent_length, df_validation['message'])\n",
        "encode_original_validate = encode_sequences(tokenizer_original_train, max_original_sent_length, df_validation['original'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtgancWqG0E6"
      },
      "source": [
        "#Building Embedded Matrix\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd3oj4juG5px"
      },
      "source": [
        "import numpy as np\n",
        "def create_embedded_matrix(embedding_matrix, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = ft.get_word_vector(word)\n",
        "    if np.all(embedding_vector) == 0:\n",
        "      embedded_matrix[index] = ft_ht.get_word_vector(word)\n",
        "      continue\n",
        "    embedding_matrix[index] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpcajwLG5uq"
      },
      "source": [
        "embedded_matrix = zeros((vocabulary_size_original, 300))\n",
        "embedded_matrix = create_embedded_matrix(embedded_matrix, tokenizer_original_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LwAFupajV9x",
        "outputId": "03d93d38-a32c-4105-d29a-30be906c539b"
      },
      "source": [
        "embedded_matrix.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9222, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-uZ9WIY0iEd"
      },
      "source": [
        "# Deleting the variables\n",
        "del ft\n",
        "del ft_ht"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzAMLbWaLNN0"
      },
      "source": [
        "#Building the Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1MpF7vELGDf"
      },
      "source": [
        "model = Sequential()\n",
        "embedding_dimension = 300\n",
        "embedding_layer = Embedding(vocabulary_size_original, embedding_dimension, weights = [embedded_matrix], input_length=max_original_sent_length\n",
        "                            , trainable = False)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F29igR7-MfNF",
        "outputId": "5299e81a-cc27-42a0-852b-a2ec28693782"
      },
      "source": [
        "embedding_layer.get_config()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activity_regularizer': None,\n",
              " 'batch_input_shape': (None, 66),\n",
              " 'dtype': 'float32',\n",
              " 'embeddings_constraint': None,\n",
              " 'embeddings_initializer': {'class_name': 'RandomUniform',\n",
              "  'config': {'maxval': 0.05, 'minval': -0.05, 'seed': None}},\n",
              " 'embeddings_regularizer': None,\n",
              " 'input_dim': 9222,\n",
              " 'input_length': 66,\n",
              " 'mask_zero': False,\n",
              " 'name': 'embedding_5',\n",
              " 'output_dim': 300,\n",
              " 'trainable': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTtAsZ0WPdqy"
      },
      "source": [
        "units = 200\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(units))\n",
        "model.add(RepeatVector(max_message_sent_length))\n",
        "model.add(LSTM(units, return_sequences=True))\n",
        "model.add(Dense(vocabulary_size_message, activation='softmax'))\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzJYWNngVfcq",
        "outputId": "2c1b2d74-7b1d-43a6-94fd-1907c601dd1b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 66, 300)           2766600   \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 200)               400800    \n",
            "_________________________________________________________________\n",
            "repeat_vector_6 (RepeatVecto (None, 57, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 57, 200)           320800    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 57, 5947)          1195347   \n",
            "=================================================================\n",
            "Total params: 4,683,547\n",
            "Trainable params: 1,916,947\n",
            "Non-trainable params: 2,766,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdcDo_gUW2k6",
        "outputId": "44cdd175-f479-4ca9-9e30-0cbe61460be8"
      },
      "source": [
        "# import numpy as np\n",
        "# count = 0\n",
        "# weights = model.layers[0].get_weights()[0]\n",
        "# for i in range(model.layers[0].get_weights()[0].shape[0]):\n",
        "#   if np.all(weights[i]) == 0:\n",
        "#     count += 1\n",
        "# print(count)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUxt4CxXXzni"
      },
      "source": [
        "filename = 'model_new.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode = 'min', verbose = 1, patience = 20)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60pshLVBX4td",
        "outputId": "63526d38-c889-42b4-fad5-b168f418e6de"
      },
      "source": [
        "history = model.fit(encode_original, encode_message.reshape(encode_message.shape[0], encode_message.shape[1], 1),\n",
        "                    batch_size=64, validation_data=(encode_original_validate, encode_message_validate),\n",
        "        verbose = 1, callbacks = [checkpoint, earlystopping], epochs = 100)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.9261\n",
            "Epoch 00001: val_loss improved from inf to 2.25673, saving model to model_new.h5\n",
            "55/55 [==============================] - 4s 66ms/step - loss: 2.9233 - val_loss: 2.2567\n",
            "Epoch 2/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.1964\n",
            "Epoch 00002: val_loss improved from 2.25673 to 2.19951, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 2.1953 - val_loss: 2.1995\n",
            "Epoch 3/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.1468\n",
            "Epoch 00003: val_loss did not improve from 2.19951\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.1476 - val_loss: 2.2369\n",
            "Epoch 4/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.1225\n",
            "Epoch 00004: val_loss did not improve from 2.19951\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.1235 - val_loss: 2.2023\n",
            "Epoch 5/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.1047\n",
            "Epoch 00005: val_loss improved from 2.19951 to 2.18133, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.1069 - val_loss: 2.1813\n",
            "Epoch 6/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0978\n",
            "Epoch 00006: val_loss improved from 2.18133 to 2.16040, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0963 - val_loss: 2.1604\n",
            "Epoch 7/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0890\n",
            "Epoch 00007: val_loss improved from 2.16040 to 2.15155, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 2.0885 - val_loss: 2.1516\n",
            "Epoch 8/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0782\n",
            "Epoch 00008: val_loss did not improve from 2.15155\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0769 - val_loss: 2.1704\n",
            "Epoch 9/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0731\n",
            "Epoch 00009: val_loss did not improve from 2.15155\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0751 - val_loss: 2.1681\n",
            "Epoch 10/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0649\n",
            "Epoch 00010: val_loss improved from 2.15155 to 2.14197, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 2.0659 - val_loss: 2.1420\n",
            "Epoch 11/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0607\n",
            "Epoch 00011: val_loss did not improve from 2.14197\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0616 - val_loss: 2.1626\n",
            "Epoch 12/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0581\n",
            "Epoch 00012: val_loss did not improve from 2.14197\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0563 - val_loss: 2.1517\n",
            "Epoch 13/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0725\n",
            "Epoch 00013: val_loss did not improve from 2.14197\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0743 - val_loss: 2.1605\n",
            "Epoch 14/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0526\n",
            "Epoch 00014: val_loss did not improve from 2.14197\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0510 - val_loss: 2.1461\n",
            "Epoch 15/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0490\n",
            "Epoch 00015: val_loss improved from 2.14197 to 2.13703, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 2.0484 - val_loss: 2.1370\n",
            "Epoch 16/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0461\n",
            "Epoch 00016: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0479 - val_loss: 2.1593\n",
            "Epoch 17/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0469\n",
            "Epoch 00017: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0440 - val_loss: 2.1694\n",
            "Epoch 18/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0397\n",
            "Epoch 00018: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0411 - val_loss: 2.1482\n",
            "Epoch 19/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0361\n",
            "Epoch 00019: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 53ms/step - loss: 2.0378 - val_loss: 2.1675\n",
            "Epoch 20/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0354\n",
            "Epoch 00020: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0359 - val_loss: 2.1429\n",
            "Epoch 21/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0322\n",
            "Epoch 00021: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0346 - val_loss: 2.1510\n",
            "Epoch 22/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0499\n",
            "Epoch 00022: val_loss did not improve from 2.13703\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 2.0475 - val_loss: 2.1524\n",
            "Epoch 23/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0555\n",
            "Epoch 00023: val_loss improved from 2.13703 to 2.13468, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 2.0547 - val_loss: 2.1347\n",
            "Epoch 24/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 2.0553\n",
            "Epoch 00024: val_loss improved from 2.13468 to 2.11917, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 2.0548 - val_loss: 2.1192\n",
            "Epoch 25/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.9898\n",
            "Epoch 00025: val_loss improved from 2.11917 to 2.06851, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 1.9899 - val_loss: 2.0685\n",
            "Epoch 26/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.9479\n",
            "Epoch 00026: val_loss did not improve from 2.06851\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.9504 - val_loss: 2.2212\n",
            "Epoch 27/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.9296\n",
            "Epoch 00027: val_loss improved from 2.06851 to 2.06211, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 1.9300 - val_loss: 2.0621\n",
            "Epoch 28/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.9204\n",
            "Epoch 00028: val_loss did not improve from 2.06211\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.9194 - val_loss: 2.0636\n",
            "Epoch 29/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.9069\n",
            "Epoch 00029: val_loss did not improve from 2.06211\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.9061 - val_loss: 2.1090\n",
            "Epoch 30/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8994\n",
            "Epoch 00030: val_loss improved from 2.06211 to 2.06170, saving model to model_new.h5\n",
            "55/55 [==============================] - 3s 56ms/step - loss: 1.8983 - val_loss: 2.0617\n",
            "Epoch 31/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8848\n",
            "Epoch 00031: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8877 - val_loss: 2.1067\n",
            "Epoch 32/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8788\n",
            "Epoch 00032: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8812 - val_loss: 2.2310\n",
            "Epoch 33/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8760\n",
            "Epoch 00033: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8755 - val_loss: 2.0825\n",
            "Epoch 34/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8725\n",
            "Epoch 00034: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8712 - val_loss: 2.0972\n",
            "Epoch 35/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8674\n",
            "Epoch 00035: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8676 - val_loss: 2.0644\n",
            "Epoch 36/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8541\n",
            "Epoch 00036: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8563 - val_loss: 2.1007\n",
            "Epoch 37/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8728\n",
            "Epoch 00037: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8715 - val_loss: 2.1001\n",
            "Epoch 38/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8487\n",
            "Epoch 00038: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8501 - val_loss: 2.1331\n",
            "Epoch 39/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8458\n",
            "Epoch 00039: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8465 - val_loss: 2.1031\n",
            "Epoch 40/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8528\n",
            "Epoch 00040: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8549 - val_loss: 2.1098\n",
            "Epoch 41/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8328\n",
            "Epoch 00041: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8335 - val_loss: 2.0849\n",
            "Epoch 42/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8335\n",
            "Epoch 00042: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8334 - val_loss: 2.0882\n",
            "Epoch 43/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8252\n",
            "Epoch 00043: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8263 - val_loss: 2.0895\n",
            "Epoch 44/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8211\n",
            "Epoch 00044: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8210 - val_loss: 2.0864\n",
            "Epoch 45/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8160\n",
            "Epoch 00045: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8170 - val_loss: 2.0923\n",
            "Epoch 46/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8174\n",
            "Epoch 00046: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8187 - val_loss: 2.0883\n",
            "Epoch 47/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8090\n",
            "Epoch 00047: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 55ms/step - loss: 1.8083 - val_loss: 2.0939\n",
            "Epoch 48/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8060\n",
            "Epoch 00048: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8067 - val_loss: 2.0982\n",
            "Epoch 49/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8044\n",
            "Epoch 00049: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8032 - val_loss: 2.1183\n",
            "Epoch 50/100\n",
            "54/55 [============================>.] - ETA: 0s - loss: 1.8119\n",
            "Epoch 00050: val_loss did not improve from 2.06170\n",
            "55/55 [==============================] - 3s 54ms/step - loss: 1.8112 - val_loss: 2.0901\n",
            "Epoch 00050: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "eVyunvl0gB1k",
        "outputId": "a42f7064-9c7f-4c51-d6e2-a658f15494bb"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8gBHswIre55"
      },
      "source": [
        "model = load_model(filename)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le_sj3zCqdpF"
      },
      "source": [
        "prediction = model.predict_classes(encode_original_validate)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4f0nTMtaaFA",
        "outputId": "c39212bb-06bd-44ec-cd52-18fa8605775e"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2, 27, 23, ...,  0,  0,  0],\n",
              "       [ 2, 27, 23, ...,  0,  0,  0],\n",
              "       [ 2, 27, 23, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 2, 27, 23, ...,  0,  0,  0],\n",
              "       [ 2, 27, 23, ...,  0,  0,  0],\n",
              "       [ 2, 27, 23, ...,  0,  0,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFbYfr00spHr"
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == n:\n",
        "      return word\n",
        "  return None\n",
        "\n",
        "predicted_text = []\n",
        "for line in prediction:\n",
        "  temp = []\n",
        "  for num in range(len(line)):\n",
        "    word = get_word(line[num], tokenizer_message_train)\n",
        "    if num > 0:\n",
        "      if (word == get_word(line[num-1], tokenizer_message_train)) or (word == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(word)\n",
        "    else:\n",
        "      if (word == None):\n",
        "        #temp.append('')\n",
        "        pass\n",
        "      else:\n",
        "        temp.append(word) \n",
        "  predicted_text.append(' '.join(temp))"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0dpx29N3zcL",
        "outputId": "44b14d29-d4cc-446b-c06a-b8b40f9014c8"
      },
      "source": [
        "predicted_text"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need a to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i need like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i am a to the                                                    ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to  the                                                   ',\n",
              " 'i would like to the                                                    ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    }
  ]
}